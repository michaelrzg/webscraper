




Feature selection for multiple classifiers

























 Next: Comparison of feature selection
 Up: Feature selection
 Previous: Frequency-based feature selection
    Contents 
    Index






Feature selection for multiple classifiers
In an operational system with a large number of classifiers,
it is desirable to select a single set of features instead
of a different one for each classifier. One way of doing
this is to compute the  statistic for an 
table where the columns are occurrence and nonoccurrence of
the term and each row corresponds to one of the classes.  We
can then select the  terms with the highest 
statistic as before.


More commonly, feature selection statistics are first
computed separately for each class on the two-class
classification task  versus  and then
combined. One combination method computes 
a single figure of
merit for each feature, for example, by averaging the values 
 for feature , and then selects the
 features with highest figures of merit. Another frequently used combination method selects
the top  features for each of  classifiers and
then combines these  sets into one global feature set.


Classification accuracy often decreases when
selecting  common features for a system with 
classifiers as opposed to  different sets of size
. But even if it does, the gain in efficiency owing to
a common document representation may be worth the loss in
accuracy .















 Next: Comparison of feature selection
 Up: Feature selection
 Previous: Frequency-based feature selection
    Contents 
    Index


© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.
2009-04-07




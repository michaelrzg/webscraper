




Results snippets

























 Next: References and further reading
 Up: Evaluation in information retrieval
 Previous: Refining a deployed system
    Contents 
    Index



 

Results snippets


Having chosen or ranked the documents matching a query, we wish to
present a results list that will be informative to the user.  In many
cases the user will not want to examine all the returned documents and
so we want to make the results list informative enough that the user
can do a final ranking of the documents for themselves based on
relevance to their information need.The standard way of doing this is to provide a  snippet , a short
summary of the document, which is designed so as to allow the user to
decide its relevance.  Typically, the snippet consists of the document title
and a short summary, which is automatically extracted. The question is
how to design the summary so as to maximize its usefulness to the
user. 


The two basic kinds of summaries are 
 static , which are always the
same regardless of the query, and  dynamic  (or
query-dependent), which are
customized according to the user's information need as deduced from a
query.  Dynamic summaries attempt to explain why a particular document
was retrieved 
for the query at hand. 


A static summary is generally comprised of either or both a subset of
the document and  metadata  associated with the document.  The simplest
form of summary takes the first two sentences or 50 words of a document,
or extracts particular zones of a document, such as the title and
author.  Instead of zones of a document, the summary can instead use
metadata associated with the document.  This may be an alternative way
to provide an author or date, or may include elements which are designed
to give a summary, such as the description metadata
which can appear in the meta element 
of a web HTML page.  This summary is typically
extracted and cached at indexing time, in such a way that it can be
retrieved and presented quickly when displaying search results, whereas
having to access the actual document content might be a relatively
expensive operation.



There has been extensive work within  natural language processing  (NLP)
on better ways to do  text summarization . Most such work still aims only
to choose sentences from the original document to present and
concentrates on how to select good sentences. 


The models typically
combine positional factors, favoring the first and last paragraphs of
documents and the first and last sentences of paragraphs, with content
factors, emphasizing sentences with key terms, which have low
document frequency in the collection as a whole, but high frequency and good
distribution across the particular document being returned.  In 
sophisticated NLP approaches, the system synthesizes sentences for a
summary, either by doing full text generation or by editing and perhaps
combining sentences used in the document.  For example, it might delete
a relative clause or replace a pronoun with the noun phrase that it
refers to.  This last class of methods
remains in the realm of research and is seldom used for search results:
it is easier, safer, and often even better to just use sentences from
the original document. 


Dynamic summaries display one or more ``windows'' on the
document, aiming to present the pieces that have the most utility to the
user in evaluating the document with respect to their information
need. Usually these windows contain one or several of the query 
terms, and so are often referred to as  keyword-in-context  (  )
snippets, though sometimes they may still be pieces of the text such as the
title 
that are selected for their query-independent information value just as in the case of static
summarization.  Dynamic summaries are generated in conjunction with
scoring. 
If the query is found as a phrase, occurrences of the phrase in the
document will be shown as the summary.  If not, windows within the document
that contain multiple query terms will be selected. 
Commonly these windows may just stretch some number of words to the left
and right of the query terms. This is a place where NLP
techniques can usefully be employed: users prefer snippets that read
well because they contain complete phrases.






Dynamic summaries are generally regarded as greatly improving the
usability of IR systems, but they 
present a complication for IR system design.  A
dynamic summary cannot be precomputed, but, on the other hand, 
if a system has only a positional index, then it cannot easily 
reconstruct the context surrounding search engine hits in order to
generate such a dynamic summary.  This is one reason for 
using static summaries.  The standard solution to this in a
world of large and cheap disk drives is to 
locally cache all the documents at index time (notwithstanding that this
approach raises various legal, information security and control
issues that are far from resolved) as shown in Figure 7.5 (page ). Then, a system can simply scan a document
which is about to appear in a
displayed results list to find snippets containing the query words.
Beyond simply access to the text, 
producing a good KWIC snippet requires some care.  Given a variety of
keyword occurrences in a document, the goal is to choose fragments which
are: (i) maximally informative about the discussion of those terms in the
document, (ii) self-contained enough to be easy to read, and
(iii) short enough to fit within the normally strict constraints on the
space available for summaries.


Generating snippets must be fast since the
system is typically generating many snippets for each query that it handles.
Rather than caching an
entire document, it is common to cache only a generous but fixed size
prefix of the document, such as perhaps 10,000 characters.  For most
common, short documents, the entire document is thus cached, but huge
amounts of local storage will not be wasted on potentially vast
documents.  Summaries of documents whose length exceeds the
prefix size will be based on material in the
prefix only, which is in general a useful zone in which to look for a
document summary  
anyway.  


If a document has been updated since it was last processed
by a crawler and indexer, these
changes will be neither in the cache nor
in the index. In these circumstances, neither the index nor the summary
will accurately reflect the current contents of the document, but it is the
differences between the summary and the actual document content that will
be more glaringly obvious to the end user.















 Next: References and further reading
 Up: Evaluation in information retrieval
 Previous: Refining a deployed system
    Contents 
    Index


© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.
2009-04-07




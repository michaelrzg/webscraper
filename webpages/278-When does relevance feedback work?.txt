




When does relevance feedback work?

























 Next: Relevance feedback on the
 Up: Relevance feedback and pseudo
 Previous: Probabilistic relevance feedback
    Contents 
    Index




When does relevance feedback work?


The success of relevance feedback depends on certain assumptions.
Firstly, the user has to have sufficient knowledge to be able to make an
initial
query which is at least somewhere close to the documents they desire.
This is needed anyhow for successful information retrieval in the basic
case, but it is important to see the kinds of problems that relevance
feedback cannot solve alone. Cases where relevance feedback alone is not
sufficient include:


Misspellings. If the user spells a term in a different way to the
  way it is spelled in any document in the collection, then relevance
  feedback is unlikely to be effective. This can be addressed by the spelling
  correction techniques of Chapter 3 .

Cross-language information retrieval. Documents in another
  language are not nearby in a vector space based on term distribution.
  Rather, documents in the same language cluster more closely together.

Mismatch of searcher's vocabulary versus collection vocabulary. If
  the user searches for laptop but all the documents use the
  term notebook computer, then the query will fail, and relevance
  feedback is again most likely ineffective.





Secondly, the relevance feedback approach requires
relevant documents to be similar to each other. That is, they should cluster.
Ideally, the term distribution in all relevant documents
will be similar to that in the documents marked by the users, while the
term distribution in all nonrelevant documents will be different from those
in relevant documents. Things will work well if
all relevant documents are tightly clustered around a single
prototype, or, at least, if there are different prototypes, if the relevant
documents have
significant vocabulary overlap, while similarities between relevant and
nonrelevant documents are small. Implicitly, the Rocchio relevance
feedback model treats relevant documents as a single
cluster, which it models via the centroid of the cluster.
This approach does not work as well if the relevant documents are a
multimodal class, 
that is, they consist
of several clusters of documents
within the vector space. This
can happen with:


Subsets of the documents using different vocabulary, such as
  Burma vs. Myanmar

A query for which the answer set is inherently disjunctive, such
  as Pop stars who once worked at Burger King.

Instances of a general concept, which often appear as a disjunction
  of more specific concepts, for example, felines.


Good editorial content in the collection can often provide a solution to
this problem. For example, an article on the attitudes of
different groups to the situation in Burma could introduce the
terminology used by different parties, thus linking the document clusters.


Relevance feedback is not necessarily popular with users. Users are
often reluctant to provide explicit feedback, or in general do not wish
to prolong the search interaction. Furthermore, it is often harder to
understand why a particular document was retrieved after relevance
feedback is applied.


Relevance feedback can also have practical problems. The long queries
that are generated by straightforward application of relevance feedback
techniques are inefficient for a typical IR system. This results in a
high computing cost for the retrieval and potentially long response
times for the user. A partial solution to this is to only reweight
certain prominent terms in the relevant documents, such as perhaps the
top 20 terms by term frequency. Some experimental results have also
suggested that using a limited number of terms like this may give better
results (Harman, 1992) though other work has suggested that
using more terms is better in terms of retrieved document quality
(Buckley et al., 1994b).















 Next: Relevance feedback on the
 Up: Relevance feedback and pseudo
 Previous: Probabilistic relevance feedback
    Contents 
    Index


© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.
2009-04-07









An example information retrieval problem

























 Next: A first take at
 Up: Boolean retrieval
 Previous: Boolean retrieval
    Contents 
    Index



 

An example information retrieval problem


A fat book which many people own is Shakespeare's Collected Works.
Suppose you wanted to determine which plays of Shakespeare contain the
words Brutus AND Caesar and NOT
Calpurnia.  One way
to do that is to start at the beginning and to read through all the
text, noting for each play whether it contains Brutus and
Caesar and excluding it from consideration if it contains
Calpurnia.
The simplest form of document retrieval is for a computer to do this
sort of linear scan
through documents.  This process is commonly referred to as
 grepping  through text, after the Unix command grep, which
performs this process.  Grepping through text can be a very effective
process, especially given the speed of modern computers, and often
allows useful possibilities for wildcard
pattern matching through the
use of   .
With modern computers, for simple querying of modest
collections (the size of Shakespeare's Collected Works is a bit under
one million words of text in total), you really need nothing more.


But for many purposes, you do need more:


To process large document collections quickly. The amount of
  online data has grown at least
as quickly as the speed of computers, and we would now like to be able
to search collections that total in the order of billions to trillions
of words.

To allow more flexible matching operations.  For example, it is
  impractical to perform the query
  Romans NEAR countrymen with grep, where
  NEAR might be defined as ``within 5 words'' or ``within the
  same sentence''.

To allow ranked retrieval: in many cases you want the best answer
  to an information need among many documents that contain certain words.



The way to avoid linearly scanning the texts for each query is
to  index  the documents in advance.  Let us stick with
Shakespeare's Collected Works, and
use it to introduce the basics of the Boolean
retrieval model.   Suppose we record for each
document - here a play of Shakespeare's - whether it contains each word
out of all the words Shakespeare used (Shakespeare used about 32,000
different words).  The result is a binary term-document 
 incidence matrix , as in Figure 1.1 .   Terms  are the indexed units (further discussed in Section 2.2 ); they are usually words, and for the moment you can think of them as words, but the information retrieval
literature normally speaks of terms because some of them, such as perhaps I-9
or Hong Kong are not usually thought of as words.
Now, depending on whether we look at the
matrix rows or columns, we can have a vector for each term, which shows
the documents it appears in, or a vector for each document, showing the
terms that occur in it.





To answer the query Brutus AND Caesar AND
NOT
Calpurnia, we take the vectors for Brutus, Caesar
and Calpurnia, complement the last, and then do a bitwise AND:

110100 AND 110111 AND 101111 = 100100


The answers for this query are thus Antony and Cleopatra
and Hamlet (Figure 1.2 ).


The
 Boolean retrieval model 
is a model for information
retrieval in which we can pose any query which is in the form of a Boolean expression of terms,
that is, in which terms are combined with
the operators and, or, and not.
The model views each document as just a set of words.




Figure:
Results from Shakespeare for the query
Brutus AND Caesar AND NOT
Calpurnia.




Let us now consider a more realistic scenario, simultaneously using the
opportunity to introduce some terminology and notation.
Suppose we have 
 documents.  By
 documents 
we mean whatever units we have decided to build a retrieval system
over.  They might be individual memos or chapters of a book (see
Section 2.1.2 (page ) for further discussion).
We will refer to the group of documents over which we
perform retrieval as the (document)  collection .  It is sometimes
also referred to as a  corpus  (a body of texts).
Suppose each document is about 1000 words long (2-3 book pages).
If we assume an average of 6 bytes per word including spaces and
punctuation, then this is a document collection about 6 GB in size.
Typically, there might be about distinct terms in these
documents.  There is nothing special about the numbers
we have chosen, and they might vary by an order of magnitude or more, but they
give us some idea of the dimensions of the kinds of problems we need to
handle.  We will discuss and model these size assumptions in
Section 5.1 (page ).


Our goal is to develop a system to address
the  ad hoc retrieval  task.  This is the most standard IR task.
In it, a system aims to provide documents from within the collection that are
relevant to an arbitrary user information need, communicated to the
system by means of a one-off, user-initiated query.  An
 information need  is the topic about which the user desires to
know more, and is differentiated from a  query , which is what
the user conveys to the computer in an attempt to communicate the
information need.
A document is  relevant  if it is
one that the user perceives as containing information of value with
respect to their personal information need.
Our example above was rather artificial in that the
information need was defined in terms of particular words, whereas
usually a user is interested in a topic like ``pipeline leaks'' and
would like to find relevant documents regardless of whether they
precisely use those words or express the concept with other words such
as pipeline rupture.
To assess the  effectiveness  of an IR system (i.e., the
quality of its search results), a user will usually want to know two key
statistics about the system's returned results for a query:


 Precision : What fraction of
the returned results are relevant to the information need?


 Recall : What fraction of the relevant documents in
  the collection were returned by the system?


Detailed discussion of relevance and evaluation measures including
precision and recall is found in Chapter 8 .


We now cannot build a term-document matrix in a naive way.
A 
 matrix has half-a-trillion 0's and 1's - too many to fit in a computer's memory.
But the crucial observation is that the matrix is extremely sparse, that
is, it has few non-zero entries.  Because each document is 1000 words
long, the matrix has no more than one
billion 1's, so a minimum of 99.8% of the cells are zero.
A much better representation is to record only the things
that do occur, that is, the 1 positions.


 
This idea is central to the first major concept in information
retrieval, the  inverted index .  The name is actually redundant:
an index always maps back from terms to the parts of a
document where they occur. Nevertheless, inverted index, or
sometimes  inverted file , has
become the standard term
in information retrieval.The basic idea of an inverted index is shown
in Figure 1.3 .  We keep a  dictionary 
of terms (sometimes also referred to as a  vocabulary  or  lexicon ; in this book, we use dictionary for the data structure and vocabulary for the set of terms).
Then for each term, we have a list that records which documents the
term occurs in. Each item in the list - which records that a term appeared in a document (and, later, often, the positions in the document) - is conventionally called
a  posting .The list is then called a  postings list 
(or   ),
and all the postings
lists taken together are referred to as the
 postings .
The dictionary in Figure 1.3  has been sorted
alphabetically and each postings list is
sorted by document ID.  We will see why this is useful in
Section 1.3 , below, but later we will
also consider
alternatives to doing this (Section 7.1.5 ).



















 Next: A first take at
 Up: Boolean retrieval
 Previous: Boolean retrieval
    Contents 
    Index


© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.
2009-04-07




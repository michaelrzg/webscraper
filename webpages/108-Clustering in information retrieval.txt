




Clustering in information retrieval

























 Next: Problem statement
 Up: Flat clustering
 Previous: Flat clustering
    Contents 
    Index




Clustering in information retrieval

The  cluster hypothesis  states
the fundamental assumption we make when using
clustering in information retrieval.

Cluster hypothesis. Documents in the same cluster
behave similarly with respect to
relevance to information needs.


The hypothesis states that if there is a document from a
cluster that is relevant to a search request, then it is
likely that other documents from the same cluster are also
relevant. This is because clustering puts 
together documents that share many terms. The
cluster hypothesis essentially is 
the contiguity hypothesis in Chapter 14 
(page 14 ). In both cases, we posit that
similar documents behave similarly with respect to
relevance.


 



Table 16.1:

Some applications of clustering in information
retrieval.

Application
What is
Benefit
Example

 
clustered?
 
 

Search result clustering
search results
more effective information
presentation to user
Figure 16.2

Scatter-Gather
(subsets of) collection
alternative
user interface: 
``search without typing''
Figure 16.3

Collection clustering
collection
effective information
presentation for exploratory browsing
McKeown et al. (2002),
http://news.google.com

Language modeling
collection
increased precision 
and/or recall
Liu and Croft (2004)

Cluster-based retrieval
collection
higher
efficiency: 
faster search
Salton (1971a)


 





Table 16.1  shows some of the main applications of
clustering in information retrieval. They differ in the set
of documents that they cluster - search results, collection or
subsets of the collection - and the aspect of an information
retrieval system they try to  improve - user experience, user
interface, effectiveness or efficiency of the
search system. But they are all based on the basic
assumption stated by the cluster hypothesis.




Clustering of search results to improve
recall. None of the top hits cover the animal sense of
jaguar, but users can easily access it by clicking on the
cat cluster in the Clustered
Results panel on the left (third arrow from the top).




The first application mentioned in Table 16.1  is
 search result
clustering  where by  search
results  we mean the documents that were returned
in response to a query.  The default presentation of search results
in information retrieval is a simple list. Users scan the
list from top to bottom until they have found the
information they are looking for. Instead, search result
clustering clusters the search results, so that similar
documents appear together. It is often easier to scan a few
coherent groups than many individual documents. This is
particularly useful if a search term has different word
senses. The example in Figure 16.2  is
jaguar. Three frequent senses on the web refer to the
car, the animal and an Apple operating system. The
Clustered Results panel returned by the
Vivísimo search engine (http://vivisimo.com) can
be a more effective user interface for understanding what is
in the search results than a simple list of documents.



An example of 
a user session in Scatter-Gather. A collection of New York
Times news stories is clustered (``scattered'') into eight
clusters (top row). The user manually gathers three of these into a
smaller collection International Stories and performs another
scattering operation. This process
repeats until a small cluster with relevant documents
is found (e.g., Trinidad).




A better user interface is also the goal of  Scatter-Gather , the second
application in Table 16.1 . Scatter-Gather clusters the whole
collection to get groups of documents that the user can
select or gather. The selected groups are merged and the
resulting set is again clustered. This process is repeated
until a cluster of interest is found. An example is shown in 
Figure 16.3 .


Automatically generated clusters like those in
Figure 16.3  are not as neatly organized as a
manually constructed hierarchical tree like the Open
Directory at http://dmoz.org. Also, finding descriptive labels for clusters
automatically is a difficult problem
(Section 17.7 , page 17.7 ). But
cluster-based navigation is an interesting alternative to
keyword searching, the standard information retrieval
paradigm. This is especially true in scenarios where users
prefer browsing over searching because they are unsure about
which search terms to use.


As an alternative to the user-mediated iterative clustering in Scatter-Gather,
we can also compute a static hierarchical
clustering of a collection that is not influenced by user
interactions (``Collection clustering'' in Table 16.1 ). Google News and its precursor, the Columbia
NewsBlaster system, are examples of this approach. In the
case of news, we need to frequently recompute the clustering
to make sure that users can access the latest breaking
stories. Clustering is well suited for access to a
collection of news stories since news reading is not really
search, but rather a process of selecting a subset of
stories about recent events.


The fourth application of clustering exploits the
cluster hypothesis directly for improving search results,
based on a clustering of the entire collection.
We use a standard inverted index to
identify an initial set of documents that match the query,
but we then
add other documents from the same clusters even if they have
low similarity to the query.
For example, if the query is car and several car
documents are taken from a cluster of automobile documents,
then we can add documents from this cluster
that use terms other than car (automobile,
vehicle etc). This can increase
recall since a group of documents with high mutual
similarity is often relevant as a whole.


More recently this idea has been used for
language modeling. 
Equation 102 ,  page 102 , showed that
to avoid sparse data problems in the language modeling
approach to IR, the 
model of document  can be interpolated with a collection model.
But the collection contains
many documents with terms untypical of . By
replacing the collection model with a model derived from
's cluster, we get more accurate estimates of the occurrence
probabilities of terms in . 


Clustering can also speed up search.  As we saw in
Section 6.3.2 
( page 6.3.2 )
search in the vector space model amounts to finding the
nearest neighbors to the query.  The inverted index supports
fast nearest-neighbor search for the standard IR setting.
However, sometimes we may not be able to use an inverted
index efficiently, e.g., in latent semantic indexing
(Chapter 18 ). In such cases, we could compute the
similarity of the query to every document, but this is
slow. The cluster hypothesis offers an alternative: Find the
clusters that are closest to the query and only consider
documents from these clusters.  Within this much smaller
set, we can compute similarities exhaustively and rank
documents in the usual way.  Since there are many fewer
clusters than documents, finding the closest cluster is
fast; and since the documents matching a query are all
similar to each other, they tend to be in the same clusters.
While this algorithm is inexact, the expected decrease in
search quality is small. This is essentially the application
of clustering that was covered in Section 7.1.6 
(page 7.1.6 ).


Exercises.

Define two documents as similar if they have at
least two proper names like Clinton or
Sarkozy in common.  Give an example of an
information need and two documents, for which the cluster
hypothesis does not hold for this notion of similarity.



Make up a simple one-dimensional example (i.e. points on a line)
with two clusters where the inexactness of cluster-based
retrieval shows up. In your example, retrieving clusters close to
the query should do worse than direct nearest neighbor search.
















 Next: Problem statement
 Up: Flat clustering
 Previous: Flat clustering
    Contents 
    Index


© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.
2009-04-07




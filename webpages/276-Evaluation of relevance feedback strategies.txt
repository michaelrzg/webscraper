




Evaluation of relevance feedback strategies

























 Next: Pseudo relevance feedback
 Up: Relevance feedback and pseudo
 Previous: Relevance feedback on the
    Contents 
    Index




Evaluation of relevance feedback strategies


Interactive relevance feedback can give very substantial gains in
retrieval performance. Empirically, one round of relevance feedback is often very useful. Two rounds is sometimes marginally more useful. Successful use of relevance feedback requires enough judged documents, otherwise the process is unstable in that it may drift away from the user's information need. Accordingly, having at least five judged documents is recommended.


There is some subtlety to evaluating the effectiveness of relevance
feedback in a sound and enlightening way. The obvious first strategy is to start with an initial query  and to compute a precision-recall graph. Following one round of feedback from the user, we compute the modified query  and again compute a precision-recall graph. Here, in both rounds we assess performance over all documents in the collection, which makes comparisons straightforward. If we do this, we find spectacular gains from relevance feedback: gains on the order of 50% in mean average precision. But unfortunately it is cheating. The gains are partly due to the fact that known relevant documents (judged by the user) are now ranked higher. Fairness demands that we should only
evaluate with respect to documents not seen by the user.


A second idea is to use documents in the residual collection (the set of documents minus those assessed relevant) for the second round of evaluation. This seems like a more realistic evaluation.
Unfortunately, the measured performance can then often be lower than for the original query. This is particularly the case if there are few relevant documents, and so a fair proportion of them have been judged by the user in the first round. The relative performance of variant relevance feedback methods can be validly compared, but it is difficult to validly compare performance with and without relevance feedback because the collection size and the number of relevant documents changes from before the feedback to after it.


Thus neither of these methods is fully satisfactory. A
third method is to have two collections, one which is used for the
initial query and relevance judgments, and the second that is then used
for comparative evaluation. The performance of both  and  can
be validly compared on the second collection.


Perhaps the best evaluation of the utility of relevance feedback is to
do user studies of its effectiveness, in particular by doing a
time-based comparison: how fast does a user find relevant documents
  with relevance feedback vs. another strategy (such as query
  reformulation), or alternatively, how many relevant documents does a
  user find in a certain amount of time. Such notions of user utility
  are fairest and closest to real system usage.















 Next: Pseudo relevance feedback
 Up: Relevance feedback and pseudo
 Previous: Relevance feedback on the
    Contents 
    Index


© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.
2009-04-07




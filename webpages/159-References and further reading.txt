




References and further reading
























 Next: Text classification and Naive
 Up: Language models for information
 Previous: Extended language modeling approaches
    Contents 
    Index



 

References and further reading


For more details on the basic concepts of probabilistic language
models and techniques for smoothing, see either  
Manning and Schütze (1999, Chapter 6) or Jurafsky and Martin (2008, Chapter 4).


The important initial papers that originated the language modeling
approach to IR are:
(Berger and Lafferty, 1999, Ponte and Croft, 1998, Miller et al., 1999, Hiemstra, 1998).
Other relevant papers can be found in the next several years of SIGIR
proceedings.  (Croft and Lafferty, 2003) contains a collection of papers from a
workshop on
language modeling approaches and Hiemstra and Kraaij (2005) review one
prominent thread of work on using language modeling approaches for TREC tasks.
Zhai and Lafferty (2001b) clarify the role of smoothing in LMs for IR
and present detailed empirical 
comparisons of different 
smoothing methods.  Zaragoza et al. (2003) advocate using full
Bayesian predictive distributions rather than MAP point estimates, but
while they outperform Bayesian smoothing, they fail to outperform a
linear interpolation.  Zhai and Lafferty (2002) argue that a two-stage
smoothing model with first Bayesian smoothing followed by linear
interpolation gives a good model of the task, and performs better and
more stably than a single form of smoothing.  A nice feature of the LM
approach is that it provides a convenient and principled way to put
various kinds of prior information into the model; Kraaij et al. (2002)
demonstrate this by showing the value of link information as a prior
in improving web entry page retrieval performance.
As briefly discussed in Chapter 16  (page 16.1 ), Liu and Croft (2004) show some gains by smoothing a document LM with
estimates from a cluster of similar documents; Tao et al. (2006)
report larger gains by doing document-similarity based smoothing.


Hiemstra and Kraaij (2005) present TREC results showing a LM approach
beating use of BM25 weights.  
Recent
work has achieved some gains by going beyond the unigram model,
providing the higher order models are smoothed with lower order models
(Cao et al., 2005, Gao et al., 2004), though the gains to date
remain modest.
Spärck Jones (2004) presents a critical viewpoint on the
rationale for the language modeling approach, but
Lafferty and Zhai (2003) argue that a unified account can be
given of the probabilistic semantics underlying both the language
modeling approach presented in this chapter and the classical
probabilistic information retrieval approach of Chapter 11 .
The Lemur Toolkit (http://www.lemurproject.org/) provides a
flexible open source framework for investigating language modeling
approaches to IR.















 Next: Text classification and Naive
 Up: Language models for information
 Previous: Extended language modeling approaches
    Contents 
    Index


© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.
2009-04-07




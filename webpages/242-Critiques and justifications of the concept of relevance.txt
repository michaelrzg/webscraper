




Critiques and justifications of the concept of relevance
























 Next: A broader perspective: System
 Up: Assessing relevance
 Previous: Assessing relevance
    Contents 
    Index



 

Critiques and justifications of the concept of relevance


The advantage of system evaluation, as enabled by the standard model of
relevant and nonrelevant documents, is that we have a fixed setting
in which we can vary IR systems and system parameters to carry out comparative
experiments.  Such formal testing is much less expensive and allows
clearer diagnosis of the effect of changing system parameters than doing user
studies of retrieval effectiveness.
Indeed, once we have a formal measure that we
have confidence in, we can
proceed to optimize effectiveness by machine learning 
methods, rather than tuning parameters by hand.  
Of course, if the formal measure poorly describes what users
actually want, doing this will not be effective in 
improving user satisfaction.  Our
perspective is that, in practice, the 
standard formal measures for IR evaluation, although a simplification,
are good enough, and 
recent work in optimizing formal evaluation measures 
in IR has succeeded brilliantly.  There are numerous examples of
techniques developed in formal evaluation settings, which improve
effectiveness in operational settings, such as the development of
document length normalization methods within the context of TREC
( and 11.4.3 ) and machine learning methods for
adjusting parameter weights in scoring (Section 6.1.2 ). 


That is not to say that there are not problems
latent within the abstractions used.  The relevance of one document is
treated as 
independent of the relevance of other documents in the collection.
(This assumption is actually built into most retrieval systems -
documents are scored against queries, not against each other - as well
as being assumed in the evaluation methods.)
Assessments are
binary: there aren't any nuanced assessments of relevance.
Relevance of a document to an information need is treated as an
absolute, objective decision. But judgments of relevance are
subjective, varying across people, as we discussed above.  In
practice, human assessors are also imperfect measuring instruments,
susceptible to failures of understanding and attention. We also have
to assume that users' 
information needs do not change as they start looking at retrieval
results.  Any results based on one collection are heavily skewed by
the choice of collection, queries, and relevance judgment set: the results 
may not translate from one domain to another or to a different user population.


Some of these problems may be fixable.  A number of recent evaluations,
including INEX, some TREC tracks, and NTCIR 
have adopted an ordinal notion of relevance with documents divided
into 3 or 4 classes, distinguishing slightly relevant documents from
highly relevant documents.  See Section 10.4 (page ) for a detailed discussion
of how this is implemented in the INEX evaluations.


One clear problem with the relevance-based assessment that we have
presented is the distinction between relevance and   marginal relevance :
whether a document still has distinctive usefulness after the user has looked
at certain other documents (Carbonell and Goldstein, 1998).  Even if a document
is highly relevant, its 
information can be completely redundant with other documents which have
already been examined.  The most extreme case of this is documents that
are duplicates - a phenomenon that is actually very common on the World
Wide Web - but it can also easily occur when several documents provide a
similar precis of an event.  In such circumstances, marginal relevance
is clearly a better measure of utility to the user.  Maximizing
marginal relevance requires returning documents that exhibit diversity
and novelty.  One way to approach measuring this is by using distinct
facts or entities as 
evaluation units.  This perhaps more directly measures true utility to
the user but doing this makes it harder to create a test collection.


Exercises.

Below is a table showing how two human judges rated the relevance of a
set of 12 documents to a particular information need (0 = nonrelevant, 1
= relevant).  Let us assume that you've written an IR system that for
this query returns the set of documents {4, 5, 6, 7, 8}.


docID
Judge 1
Judge 2

1
0
0

2
0
0

3
1
1

4
1
1

5
1
0

6
1
0

7
1
0

8
1
0

9
0
1

10
0
1

11
0
1

12
0
1




Calculate the kappa measure between the two judges.

Calculate precision, recall, and  of your system if a
  document is considered relevant only if the two judges agree.

Calculate precision, recall, and  of your system if a
  document is considered relevant if either judge thinks it is relevant.



















 Next: A broader perspective: System
 Up: Assessing relevance
 Previous: Assessing relevance
    Contents 
    Index


© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.
2009-04-07



